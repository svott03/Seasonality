# Seasonality
Seasonality Prediction on Amazon Products.

# Issues
A vast majority of data points are marked as "OOS or Low Inventory" where product sales aren't a true reflection of demand as inventory issues prevented the product from being listed during the whole duration.

# Notes
We define _good_ data points as weeks marked as "Instock" and _bad_ data points as weeks marked as "OOS or Low Inventory."

Because prices vary, we infer seasonality from the `(Date, Revenue)` relationship rather than the `(Date, Units Sold)` relationship.

# Data Insight
To find _good_ data points, we iterate over all products with a filter on the number of allowed "OOS" weeks and a filter on the number of required weeks for that product. Our `Result` field indicates the number of products that passed those filters and the `ASIN` field simply holds an `ASIN` for one of the products.

We read the first row as there only exists 1 product without any "OOS" weeks and at least 0 weeks worth of data.

Second Row: There exists 3 products with less than 10 "OOS" weeks and at least 100 weeks worth of data.

|OOS|Instances|Result|Asin|
|---|---|---|---|
|0|>0|1|B098W2T9CR|
|<10|>100|3|B07BDD7V3R|
|<20|>100|44|B076BMVCHX|
|<20|>200|21|B077FWPZLN|
|<30|>100|27|B08279T9Q5|
|<30|>200|105|B07F38QDRB|
|<30|>250|27|B073RCVR7T|
|<30|>300|5|B06XGBW6VZ|


# Imputation Problems
1. Imputation using deep learning fits some function for our `(Date, Revenue)` graph, to infer Revenue values from dates. If we were to infer some function `f : Date -> Revenue`, our seasonality results would be dishonest.

2. Linear Interpolation. We could drop all bad points and linearly interpolate between known data points. We tried this approach and got weird results. Consider a product with 6 months of _bad_ data between 2 equivalent values. If we were to drop all these data points and fit some line between the gap, our seasonal decomposition would have no information on the seasonality between this 6 month period.

3. Other Data imputation methods like MICE/RF_methods do not apply to our problem. Those methods fill in missing column values using relationships between columns. In our case, there exists no relationships between our columns.

4. Similarity. We could group products into similar categories using kNN. For each category, we then perform seasonality on _good_ data points and use that seasonality to impute missing values on _bad_ data. Unfortunately, the (Date, Revenue) graphs vastly differ between products of the same cluster, giving us no way of using _good_ data to impute _bad_ data.

# Approach
With the similarity/imputation approaches being ill-defined, we propose our own method for dealing with our Low Inventory Problem. 

We first drop all bad data points. Then add a year_percent column where `df['year_percent'] = df['sales']/ yearly_total_ij`. `Yearly_total_ij` is the total revenue generated by product `i` in year `j`. Then for every listed date of a particular product, we average the `year_percent` values over all years with good data at that time. We then perform peak finding on the `(Date, year_percent_avg)` graph. If there are bad data points for a particular year, it is likely that other years have good data at those times. Consider the image below.

![image info](./assets/Screenshot%202023-01-28%20at%208.10.41%20PM.png)

The % data drops to 0 when low inventory occurs. There exists good 2018 data where 2019 drops to 0. Likewise, 2019 has good data when 2018 drops to 0 later on. With our approach, we obtain an aggregate graph with good data throughout the year.

# Results
They appear great, now we need a way to display them on this README.
